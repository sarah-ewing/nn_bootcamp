{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/florianmuellerklein/anaconda3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/florianmuellerklein/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 9\n",
    "lstm_size = 128\n",
    "embed_size = 32\n",
    "max_word_length = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8124 words\n",
      "sandwich\n"
     ]
    }
   ],
   "source": [
    "with open('words/google-10000-english-usa-no-swears.txt', 'r') as word_file:\n",
    "    words = word_file.read().split('\\n')\n",
    "    \n",
    "# do a little bit of cleaning just in case\n",
    "def only_letters(some_string):\n",
    "    return ''.join([c for c in some_string if c in string.ascii_letters])\n",
    "\n",
    "def pad_front(some_word):\n",
    "    while len(some_word) < max_word_length:\n",
    "        some_word = '_' + some_word\n",
    "        \n",
    "    return some_word\n",
    "\n",
    "def pad_rear(some_word):\n",
    "    while len(some_word) < max_word_length:\n",
    "        some_word += '_'\n",
    "        \n",
    "    return some_word\n",
    "\n",
    "words = [only_letters(wrd) for wrd in words if 2 < len(wrd) < 10]\n",
    "    \n",
    "print('Found {} words'.format(len(words)))\n",
    "print(words[7000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____cameras >amerascay__\n",
      "cameras ('_____cameras', '>amerascay__')\n"
     ]
    }
   ],
   "source": [
    "# create program to generate pig latin\n",
    "def make_piglatin(some_word):    \n",
    "    vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "    ay = 'ay'\n",
    "\n",
    "    if some_word[0] in vowels:\n",
    "        new_word = '>' + some_word + ay\n",
    "    else:\n",
    "        new_word = '>' + some_word[1:] + some_word[0] + ay\n",
    "        \n",
    "    new_word = pad_rear(new_word)\n",
    "    old_word = pad_front(some_word)\n",
    "        \n",
    "    return old_word, new_word\n",
    "\n",
    "# test it\n",
    "old_word, new_word = make_piglatin(words[-7000])\n",
    "print(old_word, new_word)\n",
    "print(words[-7000], make_piglatin(words[-7000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get dictionaries so we can convert from letters to index and index to letters\n",
    "chars = string.ascii_letters + '>_'\n",
    "\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up the place holders for our computational graph\n",
    "inputs = tf.placeholder(shape=[1, None], dtype=tf.int32, name='encoder_input')\n",
    "decoder_inputs = tf.placeholder(shape=[None, None], dtype=tf.int32, name='decoder_input')\n",
    "targets = tf.placeholder(shape=[None, vocab_size], dtype=tf.int32, name='targets')\n",
    "\n",
    "# create an initializer to init our weight matricies\n",
    "init = tf.random_normal_initializer(stddev=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up the embedding layer\n",
    "embeddings = tf.get_variable(\"embeddings\", [vocab_size, embed_size], initializer=init)\n",
    "\n",
    "with tf.device('/cpu:0'), tf.name_scope(\"input_embedding\"):\n",
    "    encoder_embedding = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "\n",
    "with tf.device('/cpu:0'), tf.name_scope(\"output_embedding\"):\n",
    "    decoder_embedding = tf.nn.embedding_lookup(embeddings, decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# create the encoder LSTM\n",
    "with tf.variable_scope('encoder') as enc_scope:\n",
    "    # create 2 LSTMCells\n",
    "    rnn_layers = [tf.nn.rnn_cell.LSTMCell(lstm_size) for _ in range(2)]\n",
    "\n",
    "    # create a RNN cell composed sequentially of a number of RNNCells\n",
    "    multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)\n",
    "    \n",
    "    _, encoder_last_state = tf.nn.dynamic_rnn(multi_rnn_cell,\n",
    "                                              inputs=encoder_embedding, \n",
    "                                              dtype=tf.float32, \n",
    "                                              time_major=False)\n",
    "    \n",
    "# switch to our inference helper\n",
    "inference_helper = seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                 start_tokens=[char2idx['>']],\n",
    "                                                 end_token=char2idx['_'])\n",
    "\n",
    "train_helper = seq2seq.TrainingHelper(inputs=decoder_embedding, sequence_length=[11],\n",
    "                                      time_major=False)\n",
    "\n",
    "def decode(helper, scope_name, reuse=None):\n",
    "    with tf.variable_scope(scope_name, reuse=reuse) as dec_scope:\n",
    "        projection_layer = tf.layers.Dense(vocab_size, use_bias=False, name='Projection')\n",
    "        \n",
    "        multi_lstm_decoder = [tf.nn.rnn_cell.LSTMCell(lstm_size) for _ in range(2)]\n",
    "        lstm_decoder = tf.nn.rnn_cell.MultiRNNCell(multi_lstm_decoder)\n",
    "\n",
    "        decoder = seq2seq.BasicDecoder(lstm_decoder, helper, encoder_last_state, \n",
    "                                       output_layer=projection_layer)\n",
    "        \n",
    "        outputs, _, _ = seq2seq.dynamic_decode(decoder, output_time_major=False,\n",
    "                                               impute_finished=False, maximum_iterations=20)\n",
    "\n",
    "        # get the output from the decoder\n",
    "        logits = outputs.rnn_output\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "train_helper_logits = decode(train_helper, 'decoder')\n",
    "inf_helper_logits = decode(inference_helper, 'decoder', reuse=True)\n",
    "predictions = tf.argmax(tf.nn.softmax(decode(inference_helper, 'decoder', reuse=True)), -1, name='decoder_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-c5da60703751>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_entropy_th = tf.nn.softmax_cross_entropy_with_logits(labels=targets, logits=train_helper_logits)\n",
    "loss_th = tf.reduce_mean(cross_entropy_th)\n",
    "optimizer_th = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss_th)\n",
    "\n",
    "cross_entropy_ih = tf.nn.softmax_cross_entropy_with_logits(labels=targets, logits=inf_helper_logits)\n",
    "loss_ih = tf.reduce_mean(cross_entropy_ih)\n",
    "optimizer_ih = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss_ih)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.5925706624984741\n",
      "Given: ________lock, Predicted: oltlay_\n",
      "Given: _____factory, Predicted: actroryay_\n",
      "Given: ___webmaster, Predicted: ecmastarsay_\n",
      "Given: _______karma, Predicted: artalay_\n",
      "Given: _______tuner, Predicted: unertay_\n",
      "Given: _____schools, Predicted: chiorastay_\n",
      "Given: ____revealed, Predicted: eveanalsay_\n",
      "Given: _____carried, Predicted: arrinetay_\n",
      "Given: ___subscribe, Predicted: uctertiregay_\n",
      "Given: _____viewers, Predicted: ierredvay_\n",
      "\n",
      "epoch: 3, loss: 0.011746853590011597\n",
      "Given: ________lock, Predicted: ocklay_\n",
      "Given: _____factory, Predicted: actoryfay_\n",
      "Given: ___webmaster, Predicted: ebmasterway_\n",
      "Given: _______karma, Predicted: armakay_\n",
      "Given: _______tuner, Predicted: unertay_\n",
      "Given: _____schools, Predicted: choolssay_\n",
      "Given: ____revealed, Predicted: evealedray_\n",
      "Given: _____carried, Predicted: arriedcay_\n",
      "Given: ___subscribe, Predicted: ubscrbisesay_\n",
      "Given: _____viewers, Predicted: iewersvay_\n",
      "\n",
      "epoch: 6, loss: 0.0008972486830316484\n",
      "Given: ________lock, Predicted: ocklay_\n",
      "Given: _____factory, Predicted: actoryfay_\n",
      "Given: ___webmaster, Predicted: ebmasterway_\n",
      "Given: _______karma, Predicted: armakay_\n",
      "Given: _______tuner, Predicted: unertay_\n",
      "Given: _____schools, Predicted: choolssay_\n",
      "Given: ____revealed, Predicted: evealedray_\n",
      "Given: _____carried, Predicted: arriedcay_\n",
      "Given: ___subscribe, Predicted: ubscribesay_\n",
      "Given: _____viewers, Predicted: iewersvay_\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "Inference mode\n",
      "-------------------------------------\n",
      "Given: lock, Predicted: idicaltay_\n",
      "Given: factory, Predicted: octoryaay_\n",
      "Given: webmaster, Predicted: mdewubeteryay_\n",
      "Given: karma, Predicted: ketonamatay_\n",
      "Given: tuner, Predicted: tugtefeay_\n",
      "Given: schools, Predicted: ydectrounpay_\n",
      "Given: revealed, Predicted: rreverlanpay_\n",
      "Given: carried, Predicted: ycertoreday_\n",
      "Given: subscribe, Predicted: sigupntidatay_\n",
      "Given: viewers, Predicted: ebicerfespay_\n",
      "\n",
      "INFO:tensorflow:Froze 10 variables.\n",
      "Converted 10 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# get test words\n",
    "words = shuffle(words)\n",
    "test_words = words[:10]\n",
    "words = words[10:]\n",
    "\n",
    "def one_hot(v):\n",
    "    return np.eye(vocab_size)[v]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for e in range(epochs):\n",
    "        words = shuffle(words)\n",
    "        for i in range(len(words)):\n",
    "            #rng_idx = np.random.randint(len(words))\n",
    "\n",
    "            input_word, targ_word = make_piglatin(words[i])\n",
    "\n",
    "            input_vals = [char2idx[c] for c in input_word]\n",
    "            target_vals = [char2idx[c] for c in targ_word[1:]]\n",
    "            decoder_vals = [char2idx[c] for c in targ_word[:-1]]\n",
    "\n",
    "            target_vals = one_hot(target_vals)\n",
    "\n",
    "            input_vals = np.asarray(input_vals).reshape(1, len(input_vals))\n",
    "            decoder_vals = np.asarray(decoder_vals).reshape(1, len(decoder_vals))\n",
    "\n",
    "            #if np.random.random() > 0.5:\n",
    "            _, loss_val = sess.run([optimizer_th, loss_th], \n",
    "                                   feed_dict={inputs: input_vals,\n",
    "                                              decoder_inputs: decoder_vals,\n",
    "                                              targets: target_vals})\n",
    "            #else:\n",
    "            #     _, loss_val = sess.run([optimizer_ih, loss_ih], \n",
    "            #                           feed_dict={inputs: input_vals,\n",
    "            #                                      decoder_inputs: decoder_vals,\n",
    "            #                                      targets: target_vals})           \n",
    "\n",
    "\n",
    "        if e % 3 == 0:\n",
    "            print('epoch: {}, loss: {}'.format(e, loss_val))\n",
    "            #val_losses = []\n",
    "            for i in range(len(test_words)):\n",
    "                val_word, val_targ = make_piglatin(test_words[i])\n",
    "                val_input = [char2idx[c] for c in val_word]\n",
    "                val_targs = [char2idx[c] for c in val_targ[1:]]\n",
    "                val_dec_in = [char2idx[c] for c in val_targ[:-1]]\n",
    "                \n",
    "                val_targs = one_hot(val_targs)\n",
    "\n",
    "                val_input = np.asarray(val_input).reshape(1, len(val_input))\n",
    "                val_dec_in = np.asarray(val_dec_in).reshape(1, len(val_dec_in))\n",
    "\n",
    "                prediction = sess.run(predictions, feed_dict={inputs: val_input})\n",
    "\n",
    "                #val_losses.append(val_loss)\n",
    "                #prediction = np.argmax(prediction, axis=-1)\n",
    "\n",
    "                print(\"Given: {}, Predicted: {}\".format(val_word, \n",
    "                      ''.join([idx2char[idx] for idx in prediction[0]])))\n",
    "            print()\n",
    "            \n",
    "            \n",
    "    print()\n",
    "    print('-------------------------------------')\n",
    "    print('Inference mode')\n",
    "    print('-------------------------------------')\n",
    "    for i in range(len(test_words)):\n",
    "        val_word, _ = make_piglatin(test_words[i])\n",
    "        val_input = [char2idx[c] for c in val_word[i]]\n",
    "\n",
    "        val_input = np.asarray(val_input).reshape(1, len(val_input))\n",
    "        \n",
    "        # for the inference mode we only pass the english word to translate\n",
    "        prediction = sess.run(predictions, feed_dict={inputs: val_input})\n",
    "\n",
    "        print(\"Given: {}, Predicted: {}\".format(test_words[i], \n",
    "              ''.join([idx2char[idx] for idx in prediction[0]])))\n",
    "    print()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, './saved_translator/pig_latin_encoder-decoder.ckpt')\n",
    "    \n",
    "    # will save this to create a pig latin translation application, best to use frozen graph\n",
    "    relevant_nodes = ['encoder_input', 'decoder_pred']\n",
    "    output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "        sess, # The session is used to retrieve the weights\n",
    "        tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes \n",
    "        relevant_nodes # The output node names are used to select the usefull nodes\n",
    "    )\n",
    "    \n",
    "    # Finally we serialize and dump the output graph to the filesystem\n",
    "    with tf.gfile.GFile('./saved_translator/piglatin_enc-dec.pb', \"wb\") as f:\n",
    "        f.write(output_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
