{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/florianmuellerklein/anaconda3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/florianmuellerklein/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Basic Neural Networks with TF\n",
    "\n",
    "Constructing a neural network in TF low level python API allows you to create your own layers as functions to be combined in flexible ways. \n",
    "\n",
    "\n",
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/florianmuellerklein/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def normalize(data):\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    return (data - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view     ...      grade  sqft_above  \\\n",
       "0      5650     1.0           0     0     ...          7        1180   \n",
       "1      7242     2.0           0     0     ...          7        2170   \n",
       "2     10000     1.0           0     0     ...          6         770   \n",
       "3      5000     1.0           0     0     ...          7        1050   \n",
       "4      8080     1.0           0     0     ...          8        1680   \n",
       "\n",
       "   sqft_basement  yr_built  yr_renovated  zipcode      lat     long  \\\n",
       "0              0      1955             0    98178  47.5112 -122.257   \n",
       "1            400      1951          1991    98125  47.7210 -122.319   \n",
       "2              0      1933             0    98028  47.7379 -122.233   \n",
       "3            910      1965             0    98136  47.5208 -122.393   \n",
       "4              0      1987             0    98074  47.6168 -122.045   \n",
       "\n",
       "   sqft_living15  sqft_lot15  \n",
       "0           1340        5650  \n",
       "1           1690        7639  \n",
       "2           2720        8062  \n",
       "3           1360        5000  \n",
       "4           1800        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_dat = pd.read_csv('Housing_data/kc_house_data.csv')\n",
    "house_dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19451, 17)\n",
      "(19451, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load the data into numpy arrays\n",
    "housing_features = house_dat[house_dat.columns.difference(['id', 'price', 'date', 'zipcode'])].values\n",
    "housing_targets = house_dat.price.values\n",
    "housing_targets = housing_targets / np.max(housing_targets)\n",
    "\n",
    "n, f_dim = housing_features.shape\n",
    "\n",
    "# normalize the features\n",
    "housing_features= normalize(housing_features)\n",
    "\n",
    "# reshape targets to TF expectation\n",
    "housing_targets = np.expand_dims(housing_targets, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing_features,\n",
    "                                                    housing_targets,\n",
    "                                                    test_size=0.1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Neural Network Hyperparameters\n",
    "\n",
    "LR, EPOCHS, BATCHSIZE, and N_HIDDEN are all parameters that you can play with to acheive the best performance.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "EPOCHS = 200\n",
    "BATCHSIZE = 32\n",
    "N_HIDDEN = 32\n",
    "N_OUTPUT = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be training this network with mini-batch gradient descent we need to set up tensorflow placeholders for our input and target data. The place holders serve as buckets in our graph that we can place data into during training. We need only to define the shape of the data with respect to the feature dimensions. The `None` serves as a wildcard for the batchsize, having `None` there allows us to use any batch size that we choose during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, f_dim), name='X')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Layers\n",
    "\n",
    "When using one of the popular high level libraries for deep learning they typically frame the process as stacking a bunch of layers together. The layers are often set up in a way that allows you to chain an arbitrary number together. The workflow with TensorFlow is similar except that we will create our own layer functions rather than importing them from some library. \n",
    "\n",
    "Fun fact, a lot of these popular high level deep learning libraries that are built of existing frameworks started because people were building and reusing these layers functions. Some decided to release their collections of these functions as a package. A few ended up getting popular and development accelerated. \n",
    "\n",
    "When designing our layer functions there are a few things to consider. At the minimum, we should think about what the data going into our layer will be, how many neurons to have in each of our layers, what kind of activation function to use, and maybe how we want our weights to be initialized at the start of training. When using TensorFlow it's also a good idea to have names for all of our scopes within the graph. When we have to trouble shoot it will use that name in the traceback and if we view the graph in some of the TensorFlow visualization tools. \n",
    "\n",
    "In the previous notebook we set up a regression problem that we vectorized by doing a simple vector to matrix multiply. We can make that case that this is the simplest form of a neural network, in which case we should only have to perform some minor changes to create a neural network layer. What we don't want to do is a put a bunch of these single neuron functions together in parallel, this would be terribly inefficient. Instead, we want to frame our calculations as matrix operations. If we think back to that example, our weight matrix was set up with shape `[feature_dimensions, 1]` where feature_dimensions corresponds to however many columns we had in our dataset. The 1 however, we can think of as how many neurons we had in that \"layer\". So if we want 5 neurons in a layer we can change the shape of that matrix to `[feature_dimensions, 5]`. This will give us a vector of length 5 for every example that we've run through the network. \n",
    "\n",
    "```python\n",
    "def dense_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        W = tf.Variable(tf.ones([n_inputs, 5]), name='hidden_weights')\n",
    "        Z = tf.matmul(X, W)\n",
    "        out = activation(Z)\n",
    "        return out\n",
    "```\n",
    "\n",
    "Additionally, we may want to initialize our weights to something other than a matrix of ones. One popular method is to use the [He normal initialization method (https://arxiv.org/pdf/1502.01852.pdf). We can set this up quickly with two more lines and then pass the resulting `init` matrix into our `tf.Variable` weight matrix instead of starting it with `tf.ones()`.\n",
    "\n",
    "```python\n",
    "stddev = 2 / np.sqrt(n_inputs + n_units)\n",
    "init = tf.truncated_normal((n_inputs, n_units), stddev=stddev)\n",
    "```\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Build your own generic dense layer function. Note: we may want to make the activation functions optional. If we are doing a regression problem our final layer should not have an activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dense_layer(X, n_units, name, activation=None):\n",
    "    '''\n",
    "    Sets up a hidden layer to be used to build a multilayer perceptron. \n",
    "    \n",
    "    Initializes the weights of the neurons using a normal distribution withd\n",
    "    standard deviation equal to 2 / sqrt(input_dimension + number_neurons)\n",
    "    \n",
    "    Parameters:\n",
    "    X: input data for the layer\n",
    "    n_units: number of neurons to use in the layer\n",
    "    name: the name of the scope to be used with this layer\n",
    "    activation: the tensorflow nonlinearity to be used for each neuron\n",
    "    \n",
    "    Returns:\n",
    "    Tensorflow graph description representing the constructed layer\n",
    "    '''\n",
    "    with tf.name_scope(name):\n",
    "        # fill in the rest below\n",
    "        print('this will be a neural net layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_layer(X, n_units, name, activation=None):\n",
    "    '''\n",
    "    Sets up a hidden layer to be used to build a multilayer perceptron. \n",
    "    \n",
    "    Initializes the weights of the neurons using a normal distribution with\n",
    "    standard deviation equal to 2 / sqrt(input_dimension + number_neurons)\n",
    "    \n",
    "    Parameters:\n",
    "    X: input data for the layer\n",
    "    n_units: number of neurons to use in the layer\n",
    "    name: the name of the scope to be used with this layer\n",
    "    activation: the tensorflow nonlinearity to be used for each neuron\n",
    "    \n",
    "    Returns:\n",
    "    Tensorflow graph description representing the constructed layer\n",
    "    '''\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs + n_units)\n",
    "        init = tf.truncated_normal((n_inputs, n_units), stddev=stddev)\n",
    "        W = tf.Variable(init, name='hidden_weights')\n",
    "        b = tf.Variable(tf.zeros([n_units]), name='bias')\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a neural network from this layer definition is fairly straight forward now. We essentially just create a new named scope and stack two of our newly created layers together with the desired parameters. If we wanted more layers we would simply just add more to the center of our network definition.\n",
    "\n",
    "We will also create TF scopes for the loss and SGD functions, a slight twist on what we did before but the functionality is the same.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Create and train neural networks of various sizes, trying to achieve the best possible loss on the testing dataset. Using the dense layer that we created above for each layer in the newly set up neural net. \n",
    "\n",
    "#### Build the network using the 'dnn' scope below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack your layers up here\n"
     ]
    }
   ],
   "source": [
    "# build the network using the hidden layer function\n",
    "with tf.name_scope('dnn'):\n",
    "    print('Stack your layers up here')\n",
    "    \n",
    "# set up the optimizer and loss function below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network using mini-batch gradient descent. We will slice up our training data into batches of size `BATCHSIZE` and feed each batch into the network in succession. Just like we did with the linear model in the previous notebook, but with smaller portions of the dataset for each step rather than the whole thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run the TF session with the batches and a feed_dict\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "n_samples = X_train.shape[0]\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    print('Run the TF session with the batches and a feed_dict')\n",
    "    \n",
    "    for e in range(EPOCHS):\n",
    "            \n",
    "        for i in range((n_samples + BATCHSIZE - 1) // BATCHSIZE):\n",
    "            sl = slice(i * BATCHSIZE, (i+1) * BATCHSIZE)\n",
    "            X_b = X_train[sl]\n",
    "            y_b = y_train[sl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Current loss: 0.016239671 Test loss: 0.016392678\n",
      "Epoch: 10 Current loss: 0.0018245125 Test loss: 0.0019125378\n",
      "Epoch: 20 Current loss: 0.0013403709 Test loss: 0.001126317\n",
      "Epoch: 30 Current loss: 0.001247742 Test loss: 0.00091896765\n",
      "Epoch: 40 Current loss: 0.0012386965 Test loss: 0.0008277681\n",
      "Epoch: 50 Current loss: 0.0012298102 Test loss: 0.0007808545\n",
      "Epoch: 60 Current loss: 0.0012109921 Test loss: 0.00074711017\n",
      "Epoch: 70 Current loss: 0.0011943849 Test loss: 0.00072223996\n",
      "Epoch: 80 Current loss: 0.0011806503 Test loss: 0.00070356025\n",
      "Epoch: 90 Current loss: 0.0011695836 Test loss: 0.0006882964\n",
      "Epoch: 100 Current loss: 0.0011602318 Test loss: 0.0006756215\n",
      "Epoch: 110 Current loss: 0.0011517017 Test loss: 0.0006646552\n",
      "Epoch: 120 Current loss: 0.0011446918 Test loss: 0.0006550582\n",
      "Epoch: 130 Current loss: 0.0011388522 Test loss: 0.00064631586\n",
      "Epoch: 140 Current loss: 0.0011332888 Test loss: 0.00063866336\n",
      "Epoch: 150 Current loss: 0.0011284952 Test loss: 0.0006317993\n",
      "Epoch: 160 Current loss: 0.0011238833 Test loss: 0.0006256172\n",
      "Epoch: 170 Current loss: 0.001119698 Test loss: 0.00061998545\n",
      "Epoch: 180 Current loss: 0.0011158438 Test loss: 0.00061482115\n",
      "Epoch: 190 Current loss: 0.0011120788 Test loss: 0.00061005406\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('dnn'):\n",
    "    hidden_1 = dense_layer(X, N_HIDDEN, name='hidden_layer', activation=tf.nn.relu)\n",
    "    hidden_2 = dense_layer(hidden_1, N_HIDDEN, name='hidden_layer', activation=tf.nn.relu)\n",
    "    y_pred = dense_layer(hidden_2, N_OUTPUT, name='output')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    error = y_pred - y\n",
    "    loss = tf.reduce_mean(tf.square(error), name='mse')\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.GradientDescentOptimizer(LR).minimize(loss)\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for e in range(EPOCHS):\n",
    "            \n",
    "        for i in range((n_samples + BATCHSIZE - 1) // BATCHSIZE):\n",
    "            sl = slice(i * BATCHSIZE, (i+1) * BATCHSIZE)\n",
    "            X_b = X_train[sl]\n",
    "            y_b = y_train[sl]\n",
    "            _, train_loss = sess.run([train_step, loss], feed_dict={X: X_b, y: y_b})\n",
    "            losses.append(train_loss)\n",
    "            \n",
    "        if e % 10 == 0:\n",
    "            print(\"Epoch:\", e, \n",
    "                  \"Current loss:\", train_loss, \n",
    "                  \"Test loss:\",\n",
    "                  sess.run(loss, feed_dict={X: X_test, y: y_test}))\n",
    "        \n",
    "    save_path = saver.save(sess, '/tmp/mlp_regression.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Load the saved model and make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/mlp_regression.ckpt\n",
      "0.0006061288\n"
     ]
    }
   ],
   "source": [
    "# best weight values are saved in the checkpoint\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, '/tmp/mlp_regression.ckpt')\n",
    "    preds = sess.run(y_pred, feed_dict={X: X_test})\n",
    "    print(sess.run(loss, feed_dict={X: X_test, y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did better than the simple linear regression from the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfhoUlFJK+f1/uiPiTYK9wsgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a22c6c668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_test, preds)\n",
    "ax.plot([y_test.min(), y_test.max()], \n",
    "        [y_test.min(), y_test.max()], '--', lw=3, color='r')\n",
    "ax.set_xlabel('True Value')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
