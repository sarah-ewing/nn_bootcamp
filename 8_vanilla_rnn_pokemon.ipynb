{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florian/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating fake Pokemon names with RNNs\n",
    "\n",
    "An interesting property of recurrent neural networks is their ability to generate sequences. We see this when using an RNN for image captioning, language translation, sequence tagging, or sequence forecasting. To help understand this method we'll use a character level RNN model the sequences of letters that make up a Pokemon's name and type. A technique popularized by Andre Karpathy in his blog post [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/). \n",
    "\n",
    "## Load the provided Pokemon csv file with pandas and view it's structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>Total</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bulbasaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>318</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ivysaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>405</td>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>525</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>VenusaurMega Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>625</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>123</td>\n",
       "      <td>122</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Charmander</td>\n",
       "      <td>Fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>309</td>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>43</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #                   Name Type 1  Type 2  Total  HP  Attack  Defense  \\\n",
       "0  1              Bulbasaur  Grass  Poison    318  45      49       49   \n",
       "1  2                Ivysaur  Grass  Poison    405  60      62       63   \n",
       "2  3               Venusaur  Grass  Poison    525  80      82       83   \n",
       "3  3  VenusaurMega Venusaur  Grass  Poison    625  80     100      123   \n",
       "4  4             Charmander   Fire     NaN    309  39      52       43   \n",
       "\n",
       "   Sp. Atk  Sp. Def  Speed  Generation  Legendary  \n",
       "0       65       65     45           1      False  \n",
       "1       80       80     60           1      False  \n",
       "2      100      100     80           1      False  \n",
       "3      122      120     80           1      False  \n",
       "4       60       50     65           1      False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in some text to use\n",
    "poke_df = pd.read_csv('pokemon/Pokemon.csv')\n",
    "poke_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a list of strings of just pokemon names and their types\n",
    "\n",
    "We append an 'End of Sentence' token to each pokemon name. When we end up generating the pokemon names the network will output that EOS token when it's done making up a name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulbasaur Grass<EOS>\n"
     ]
    }
   ],
   "source": [
    "def strip_non_ascii(some_string):\n",
    "    return ''.join([c for c in some_string if c in string.printable])\n",
    "\n",
    "pokemons = [\"{} {}<EOS>\".format(df_row[1]['Name'], df_row[1]['Type 1']) for df_row in poke_df.iterrows()]\n",
    "pokemons = [strip_non_ascii(pokemon) for pokemon in pokemons]\n",
    "print(pokemons[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for creating training data\n",
    "\n",
    "For this task we will go over each character in a string and train the RNN to predict the following character. So set up our input and targets for this type of training we simply take all but the last character for the input and all but the first character for the output. Then we'll pair up those sequences for our (input, target) pairs. \n",
    "\n",
    "This will be effective because once we have the model trained we can generate fake names by starting with a randomly generated letter and feeding that to the network. The network will then predict a reasonable next character, which is then fed back into the network to generate the next letter. This goes on until we reach the EOS token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_iter(pokemon_names):\n",
    "    inp = pokemon_names[:-1] # all but last\n",
    "    targ = pokemon_names[1:] # all but first\n",
    "    \n",
    "    return inp, targ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks can not work on characters directly. Instead we need some numerical representation of those characters. So we'll make a list of every printable character and use that to create dictionaries to convert a letter to an integer and another to convert back to the character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is 800 pokemon long and there are 100 unique characters.\n"
     ]
    }
   ],
   "source": [
    "# need to get all of the possible characters that the source uses\n",
    "chars = string.printable\n",
    "\n",
    "data_size, vocab_size = len(pokemons), len(chars)\n",
    "print('Text is', data_size, 'pokemon long and there are', vocab_size, 'unique characters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dictionaries to convert from characters to index and from index back to characters\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2char = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an RNN in tensorflow\n",
    "\n",
    "In this notebook we'll work in Tensorflow directly. I would recommend getting familiar with how neural networks work by using our previous examples and then once you feel comfortable with Keras and all of the high level concepts move into Tensorflow. \n",
    "\n",
    "Tensorflow does give us a few helper functions to facilitate the construction of neural networks, but mostly we will be building lots of things from scratch. The one thing that we definitely don't want to do is calculate the backward pass for our training steps, luckily this is something that Tensorflow will do for us. \n",
    "\n",
    "In this example we will create a GRU recurrent neural network to use in our character level RNN. The steps for creating this network from scratch will be:\n",
    "\n",
    "* Initialize all of our weight matrices. Setup their sizes and fill with random numbers\n",
    "* Define the calculations that our network must carry out\n",
    "\n",
    "A GRU cell is basically a change in the way that the hidden state is calculated for a recurrent neural network. So to begin we'll start with a vanilla recurrent neural network and show how we can create one using the two steps above. \n",
    "\n",
    "### Vanilla RNN\n",
    "\n",
    "The calculations for a recurrent neural network look like the following:\n",
    "\n",
    "![rnn](images/rnn.png)\n",
    "\n",
    "In order to create that we need to set up three matrices and two bias vectors. The specify the calculations in exactly the same way. \n",
    "\n",
    "```python\n",
    "Uh = tf.get_variable(\"Uh\", [input_size, hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "Wh = tf.get_variable(\"Wh\", [hidden_size, hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "Vy = tf.get_variable(\"Vy\", [hidden_size, vocab_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "bh  = tf.get_variable(\"bh\", [hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "by  = tf.get_variable(\"by\", [output_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "\n",
    "hs_t = tf.tanh(tf.matmul(xs_t, Uh) + tf.matmul(hs_t, Wh) + bh)\n",
    "ys_t = tf.nn.softmax(tf.matmul(hs_t, Vy) + by)\n",
    "```\n",
    "\n",
    "Simple enough. Input_size and output_size will change depending on the properties of our data. Hidden_size is a hyperparameter that we can set to anything that we wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define some hyperparameters for our network\n",
    "embed_size = 64\n",
    "hidden_size = 128\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Set up the RNN and Train it to generate pokemon names\n",
    "\n",
    "Whenever we run a RNN for a step we get two outputs. The output of the RNN and it's current hidden state. On the next step, we have to pass both the data at that step and the hidden state from the previous step. Therefore, we need to set up a placeholder to recieve the hiddent states at each step. This will be like having two inputs to our network, both the input and hidden state need to be fed with the feed_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up the place holders for our computational graph\n",
    "inputs = tf.placeholder(shape=[None, 1], dtype=tf.int32, name='input')\n",
    "targets = tf.placeholder(shape=[None, vocab_size], dtype=tf.float32, name='targets')\n",
    "init_state = tf.placeholder(shape=[1, hidden_size], dtype=tf.float32, name='state')\n",
    "\n",
    "# create an initializer to init our weight matricies\n",
    "init = tf.random_normal_initializer(stddev=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "Passing integers to a neural network is not very useful. All of our operations are typically matrix operations of some kind. Performing those operations with an integer does not give us a lot of flexibility in the kinds of representations that the network can learn. Instead we will put something called and `embedding layer` at the start of our network. For this purpose this will be of size `[vocab_size, embedding size]`, where vocab is the number of possible characters and embedding size is a hyper parameter. This matrix will be initialized with random numbers. The embedding layer will be a lookup table. The index representing the character will correspond to the index of some vector within that matrix. That vector will then be used to represent our character to the network. This way we perform a vector-matrix operation when processing a character. The values in the embedding matrix will update through backpropagation just like the rest of the network. They will end up becoming meaningful vector representations of the characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up the embedding layer\n",
    "with tf.device('/cpu:0'), tf.name_scope(\"Embedding\"):\n",
    "    embedding = tf.get_variable(\"embedding\", [vocab_size, embed_size], initializer=init)\n",
    "    inputs_embedding = tf.nn.embedding_lookup(embedding, inputs)\n",
    "    inputs_embedding = tf.reshape(inputs_embedding, (1, embed_size))\n",
    "\n",
    "# set up our recurrent neural network and define the functions\n",
    "with tf.variable_scope(\"RNN\") as scope:\n",
    "    # hidden state at time t \n",
    "    hs_t = init_state\n",
    "            \n",
    "    Uh = tf.get_variable(\"Uh\", [embed_size, hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "    Wh = tf.get_variable(\"Wh\", [hidden_size, hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "    Vy = tf.get_variable(\"Vy\", [hidden_size, vocab_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "    bh  = tf.get_variable(\"bh\", [hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "    by  = tf.get_variable(\"by\", [vocab_size], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "\n",
    "    hs_t = tf.tanh(tf.matmul(inputs_embedding, Uh) + tf.matmul(hs_t, Wh) + bh)\n",
    "    ys_t = tf.matmul(hs_t, Vy) + by\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-995dd560feab>:7: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# need to keep track of our hidden states\n",
    "h_0 = hs_t\n",
    "# apply the softmax output to the last output of our list\n",
    "output_softmax = tf.nn.softmax(ys_t)\n",
    "\n",
    "# get all of the output characters together\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=targets, logits=ys_t))\n",
    "\n",
    "# optimization algorithm\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0005)\n",
    "grads = optimizer.compute_gradients(loss)\n",
    "\n",
    "# clip the gradients\n",
    "grad_clipping = tf.constant(5.0, name='grad_clipping')\n",
    "clipped_grads = []\n",
    "for grad, var in grads:\n",
    "    clipped_grad = tf.clip_by_value(grad, -grad_clipping, grad_clipping)\n",
    "    clipped_grads.append((clipped_grad, var))\n",
    "    \n",
    "# update the weights with gradient descent\n",
    "updates = optimizer.apply_gradients(clipped_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, p: 0, loss: 4.160430\n",
      "----\n",
      " O;ErarDEE;rpBrEzr;\t:ErjEE.r Br6zj;E;Fr;B>.r;\tJ;rjBr \n",
      "----\n",
      "\n",
      "iter: 1000, p: 0, loss: 0.000246\n",
      "----\n",
      " Dadolos Electric \n",
      "----\n",
      "\n",
      "iter: 2000, p: 0, loss: 0.000025\n",
      "----\n",
      " Holie Ice \n",
      "----\n",
      "\n",
      "iter: 3000, p: 0, loss: 0.000108\n",
      "----\n",
      " Carditord Water \n",
      "----\n",
      "\n",
      "iter: 4000, p: 0, loss: 0.000014\n",
      "----\n",
      " Flant Rock \n",
      "----\n",
      "\n",
      "iter: 5000, p: 0, loss: 0.000010\n",
      "----\n",
      " Quctrote Fire \n",
      "----\n",
      "\n",
      "iter: 6000, p: 0, loss: 0.000005\n",
      "----\n",
      " Zunt Ice \n",
      "----\n",
      "\n",
      "iter: 7000, p: 0, loss: 0.000438\n",
      "----\n",
      " Non Grass \n",
      "----\n",
      "\n",
      "iter: 8000, p: 0, loss: 0.000001\n",
      "----\n",
      " Fle Size Ghost \n",
      "----\n",
      "\n",
      "iter: 9000, p: 0, loss: 0.000003\n",
      "----\n",
      " Gradoss Electric \n",
      "----\n",
      "\n",
      "iter: 10000, p: 0, loss: 0.000000\n",
      "----\n",
      " Fine Ghost \n",
      "----\n",
      "\n",
      "iter: 11000, p: 0, loss: 0.000013\n",
      "----\n",
      " Kino Normal \n",
      "----\n",
      "\n",
      "iter: 12000, p: 0, loss: 0.000000\n",
      "----\n",
      " Vicoom Dark \n",
      "----\n",
      "\n",
      "iter: 13000, p: 0, loss: 0.000002\n",
      "----\n",
      " OSwin Fire \n",
      "----\n",
      "\n",
      "iter: 14000, p: 0, loss: 0.000002\n",
      "----\n",
      " Qlepich Ice \n",
      "----\n",
      "\n",
      "iter: 15000, p: 0, loss: 0.000008\n",
      "----\n",
      " Hoad Formal \n",
      "----\n",
      "\n",
      "iter: 16000, p: 0, loss: 0.000000\n",
      "----\n",
      " Jive Size Ghost \n",
      "----\n",
      "\n",
      "iter: 17000, p: 0, loss: 0.000006\n",
      "----\n",
      " Nollos Grass \n",
      "----\n",
      "\n",
      "iter: 18000, p: 0, loss: 0.000040\n",
      "----\n",
      " Zill Ice \n",
      "----\n",
      "\n",
      "iter: 19000, p: 0, loss: 0.000001\n",
      "----\n",
      " Voikoud Water \n",
      "----\n",
      "\n",
      "iter: 20000, p: 0, loss: 0.000016\n",
      "----\n",
      " Pink Psychic \n",
      "----\n",
      "\n",
      "iter: 21000, p: 0, loss: 0.000000\n",
      "----\n",
      " Nuson \n",
      "----\n",
      "\n",
      "iter: 22000, p: 0, loss: 0.000034\n",
      "----\n",
      " Jte Ghost \n",
      "----\n",
      "\n",
      "iter: 23000, p: 0, loss: 0.000001\n",
      "----\n",
      " Flagino Normal \n",
      "----\n",
      "\n",
      "iter: 24000, p: 0, loss: 0.000000\n",
      "----\n",
      " Yadria Dragon \n",
      "----\n",
      "\n",
      "iter: 25000, p: 0, loss: 0.000003\n",
      "----\n",
      " Vus Dragon \n",
      "----\n",
      "\n",
      "iter: 26000, p: 0, loss: 0.000017\n",
      "----\n",
      " Ylele Ghost \n",
      "----\n",
      "\n",
      "iter: 27000, p: 0, loss: 0.000001\n",
      "----\n",
      " Gat Psychic \n",
      "----\n",
      "\n",
      "iter: 28000, p: 0, loss: 0.000008\n",
      "----\n",
      " Stzeongour Rock \n",
      "----\n",
      "\n",
      "iter: 29000, p: 0, loss: 0.000000\n",
      "----\n",
      " Fleggon Ground \n",
      "----\n",
      "\n",
      "iter: 30000, p: 0, loss: 0.000000\n",
      "----\n",
      " Jty Dragon \n",
      "----\n",
      "\n",
      "iter: 31000, p: 0, loss: 0.000007\n",
      "----\n",
      " Woomad Normal \n",
      "----\n",
      "\n",
      "iter: 32000, p: 0, loss: 0.000000\n",
      "----\n",
      " ZorusIncarnate Fighting \n",
      "----\n",
      "\n",
      "iter: 33000, p: 0, loss: 0.000000\n",
      "----\n",
      " Darmo Grass \n",
      "----\n",
      "\n",
      "iter: 34000, p: 0, loss: 0.000001\n",
      "----\n",
      " ValieMega Gholtele Ghost \n",
      "----\n",
      "\n",
      "iter: 35000, p: 0, loss: 0.000000\n",
      "----\n",
      " Sherporin Fire \n",
      "----\n",
      "\n",
      "iter: 36000, p: 0, loss: 0.000000\n",
      "----\n",
      " Meevink Ghost \n",
      "----\n",
      "\n",
      "iter: 37000, p: 0, loss: 0.000000\n",
      "----\n",
      " Narpadosom Grass \n",
      "----\n",
      "\n",
      "iter: 38000, p: 0, loss: 0.000000\n",
      "----\n",
      " Ineton Grass \n",
      "----\n",
      "\n",
      "iter: 39000, p: 0, loss: 0.000000\n",
      "----\n",
      " Inslaud Water \n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now that all the functions are set up we can run this thing\n",
    "\n",
    "# function to one hot encode the characters\n",
    "def one_hot(v):\n",
    "    return np.eye(vocab_size)[v]\n",
    "\n",
    "# Session\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Initial values\n",
    "n, p = 0, 0\n",
    "#hprev_val = np.zeros([1, hidden_size])\n",
    "\n",
    "for _ in range(epochs):\n",
    "    for pokemon in pokemons:\n",
    "        # Initialize the hidden state to 0 at the beginning of each sequence\n",
    "        h_t = np.zeros([1, hidden_size])\n",
    "\n",
    "        # Prepare inputs\n",
    "        input_vals, target_vals = list_iter(pokemon)\n",
    "\n",
    "        input_vals = [char2idx[c] for c in input_vals]\n",
    "        target_vals = [char2idx[c] for c in target_vals]\n",
    "\n",
    "        target_vals = one_hot(target_vals)\n",
    "   \n",
    "        losses = []\n",
    "        for c in range(len(input_vals)):\n",
    "            # run the tensorflow session\n",
    "            h_t, loss_val, _ = sess.run([h_0, loss, updates],\n",
    "                                        feed_dict={inputs: np.asarray(input_vals[c]).reshape(1,1),\n",
    "                                                   targets: target_vals[c].reshape(1,100),\n",
    "                                                   init_state: h_t})\n",
    "        losses.append(loss_val)\n",
    "        if n % 1000 == 0:\n",
    "            # Progress\n",
    "            print('iter: %d, p: %d, loss: %f' % (n, p, np.mean(losses)))\n",
    "\n",
    "            # Do sampling\n",
    "            sample_length = 50\n",
    "            prime_str_idx = np.random.randint(len(string.ascii_uppercase))\n",
    "            prime_str = string.ascii_uppercase[prime_str_idx]\n",
    "\n",
    "            idxs = []\n",
    "            sample_prev_state_val = np.copy(h_t)\n",
    "            sample_input_vals = np.asarray([char2idx[prime_str]]).reshape(1,1)\n",
    "\n",
    "            for t in range(sample_length):\n",
    "                sample_output_softmax_val, sample_prev_state_val = \\\n",
    "                    sess.run([output_softmax, h_0],\n",
    "                             feed_dict={inputs: sample_input_vals, init_state: sample_prev_state_val})\n",
    "\n",
    "                predicted_idx = (np.argmax(sample_output_softmax_val))\n",
    "\n",
    "                idxs.append(predicted_idx)\n",
    "                sample_input_vals = np.asarray([predicted_idx]).reshape(1,1)\n",
    "\n",
    "            txt = prime_str + ''.join(idx2char[ix] for ix in idxs)\n",
    "            print('----\\n %s \\n----\\n' % (txt.split('<EOS>')[0],))\n",
    "\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
